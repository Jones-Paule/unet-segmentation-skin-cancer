{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "900 labeled Image(.jpg),GroundTruth(.png) pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "I'm using the Unet Architecture as described the paper [here](https://arxiv.org/pdf/1505.04597.pdf).<br/>\n",
    "The tensorflow implementation of the same is found [here](https://github.com/jakeret/tf_unet).<br/>\n",
    "Keras implememtation [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation).<br/>\n",
    "I'll be using the tensorflow implemetation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "+ **The tf_unet package requires having both the Input and groundtruth images in the same folder. **\n",
    "\n",
    "  I wrote a simple [shell script](https://gist.github.com/kaushiksk/cea2b001666f4fe100b2ba31afea3eb0#file-script-sh) to create a 600-300 split and added corresponding input and groundtruth images to the **train/** and **test/** folders.\n",
    "\n",
    "\n",
    "+ **The tf_unet package requries having all images of the same height and width to be fed into the network**\n",
    "\n",
    "  I used ImageMagick's mogrify tool for this.\n",
    "  \n",
    "  ```\n",
    "  $ mogrify -resize 900*700! *.jpg\n",
    "$ mogrify -resie 900*700! *.png\n",
    "  ```\n",
    "  \n",
    "  I discared one image from the training set as it's size was smaller than this. That left me with 599 training images and 300 test images of size 900\\*700.\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Images to greyscale\n",
    "From the last experiment it was clear that the classification was biased towards the red channel, here I convert all input images to greyscale because all necessary informationcan in fact be captured in a sinvle channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert_to_grey.py\n",
    "# The following script is run both in the train/ and the test/ folder\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "imgs = glob.glob(\"*.jpg\")\n",
    "\n",
    "for file in imgs:\n",
    "    img =  Image.open(file).convert('L')\n",
    "    filename = file.split('.')[0] + \"_gray.jpg\"\n",
    "    img.save(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I made a few changes to the model, increasing the depth and root features and limiting inputs to a single channel\n",
    "\n",
    "tf_unet docs are available [here](http://tf-unet.readthedocs.io/en/latest/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-17 11:45:16,867 Layers 4, features 32, filter size 3x3, pool size: 2x2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files used: 599\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from tf_unet import unet, util, image_util\n",
    "\n",
    "# this is a helper object provided by the module to feed data to the network. The parameters are self explanatory\n",
    "# docs : http://tf-unet.readthedocs.io/en/latest/tf_unet.html#tf_unet.image_util.ImageDataProvider\n",
    "\n",
    "data_provider = image_util.ImageDataProvider(\"./train/train/*\",data_suffix=\"gray.jpg\",mask_suffix=\"Segmentation.png\")\n",
    "\n",
    "output_path = \"./model_val_20_epochs/\"\n",
    "\n",
    "#setup & training\n",
    "net = unet.Unet(layers=4, features_root=32, channels=1, n_class=2)\n",
    "trainer = unet.Trainer(net)\n",
    "path = trainer.train(data_provider, output_path, training_iters=32, epochs=20,restore=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Contd.\n",
    "\n",
    "I put the above code in a .py file and ran it for about 20 epochs.<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "I interrupted training multiple times to observe what was being output by the network. Read through the source code extensively as well.\n",
    "\n",
    "The model output a greyscale image which was just the \"white\" predictions scaled to 255.\n",
    "\n",
    "Each pixel was a binary softmax of the type [black_prob, white_prob] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_val_20_epochs/model.cpkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-17 11:45:25,536 Restoring parameters from ./model_val_20_epochs/model.cpkt\n",
      "2017-07-17 11:45:25,773 Model restored from file: ./model_val_20_epochs/model.cpkt\n"
     ]
    }
   ],
   "source": [
    "# Retrive one test example\n",
    "# Retrive stored model (This is saved after training automatically)\n",
    "# predict on the test input\n",
    "\n",
    "x_test, y_test = data_provider(1)\n",
    "path = \"./model_val_20_epochs/model.cpkt\"\n",
    "prediction = net.predict(path, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 700, 900, 1), (1, 700, 900, 2), (1, 612, 812, 2))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape,y_test.shape,prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output prediction is indeed smaller than the input images. This is due to the various levels of convolution.<br/>\n",
    "Error is calculated in the model by cropping the groundtruth to the size of predicted image, and then comparing the white channel of the groundtruth with the argmax(pixelwise softmax) of the predicted image.\n",
    "\n",
    "The model does internal processing of the groundtruth images to represent each pixel as [1,0] or [0,1].\n",
    "\n",
    "###### I played around with the output here a lot and came up with the below helper functions to represent and predict the ouput on the entire training set.\n",
    "\n",
    "#### I also tried different thresholds for the output image and found that 0.5 gave the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 900)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0,...,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the next test_x, test_y pair\n",
    "# This is just a slightly modified version of the function in the source code\n",
    "\n",
    "def getnext(data_provider):\n",
    "    data, label = data_provider._next_data()\n",
    "\n",
    "    train_data = data_provider._process_data(data)\n",
    "    labels = data_provider._process_labels(label)\n",
    "\n",
    "    train_data, labels = data_provider._post_process(train_data, labels)\n",
    "\n",
    "    nx = data.shape[1]\n",
    "    ny = data.shape[0]\n",
    "\n",
    "    return train_data.reshape(1, ny, nx, data_provider.channels), labels.reshape(1, ny, nx, data_provider.n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to store images in two folders respectively depending on the accuracy\n",
    "\n",
    "def visualize(x_test,y_test,prediction,i,err=False):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 4, sharex=True, sharey=True, figsize=(12,5))\n",
    "    \n",
    "    ax[0].imshow(x_test[0,...,-1], aspect=\"auto\", cmap=\"gray\") #input\n",
    "    ax[1].imshow(y_test[0,...,1], aspect=\"auto\",cmap=\"gray\") #groundtruth\n",
    "    ax[2].imshow(prediction[0,...,1], aspect=\"auto\",cmap=\"gray\") #prediciton\n",
    "        \n",
    "    mask = ((prediction[0,..., 1]>=0.4).astype('float32')*255).astype(np.uint8) # applying threshold on white channel of prediction\n",
    "    ax[3].imshow(mask, aspect=\"auto\",cmap=\"gray\")\n",
    "    \n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].set_title(\"Ground truth\")\n",
    "    ax[2].set_title(\"Prediction\")\n",
    "    ax[3].set_title(\"Prediction > 0.4\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if err:\n",
    "        fig.savefig(\"./outliers/pred_{}.png\".format(i))\n",
    "    else:\n",
    "        fig.savefig(\"./bestfits/pred_{}.png\".format(i))   \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below line to see what the function does. It basically puts together images side by side and saves them as a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(x_test,y_test,prediction,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./bestfits/pred_1.png\">\n",
    "\n",
    "### Evaluation on training set\n",
    "\n",
    "I use the same improvememt I used in the previous experiment for evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwriting the inbuilt functions\n",
    "\n",
    "I went through the netire source code at this point to know where exactly I had to make changes. I realised I could fix the overhead computations by changing just two functions and then running things inside my own session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tf_unet.unet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inheriting the unet.Unet class and overriding a few functions.\n",
    "# source code : http://tf-unet.readthedocs.io/en/latest/_modules/tf_unet/unet.html#Unet\n",
    "\n",
    "class myUnet(unet.Unet):\n",
    "\n",
    "\n",
    "    def predict(self, sess, model_path, x_test,restore=True):\n",
    "        \"\"\"\n",
    "        Uses the model to create a prediction for the given data\n",
    "        \n",
    "        :param model_path: path to the model checkpoint to restore\n",
    "        :param x_test: Data to predict on. Shape [n, nx, ny, channels]\n",
    "        :returns prediction: The unet prediction Shape [n, px, py, labels] (px=nx-self.offset/2) \n",
    "        \"\"\"\n",
    "\n",
    "        # Restore model weights from previously saved model before calling this\n",
    "\n",
    "        y_dummy = np.empty((x_test.shape[0], x_test.shape[1], x_test.shape[2], self.n_class))\n",
    "        prediction = sess.run(self.predicter, feed_dict={self.x: x_test, self.y: y_dummy, self.keep_prob: 1.})\n",
    "            \n",
    "        return prediction\n",
    "\n",
    "\n",
    "    def restore(self, sess, model_path):\n",
    "        \"\"\"\n",
    "        Restores a session from a checkpoint\n",
    "        \n",
    "        :param sess: current session instance\n",
    "        :param model_path: path to file system checkpoint location\n",
    "        \"\"\"\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "        logging.info(\"Model restored from file: %s\" % model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now good to go. Change of metric and change of way of representing the ouput as well. <br/>\n",
    "I decided it would help to display the difference between the predicted and groundtruth images in black and white as this would allow me to see exactly which pixels were being classified wrongly.\n",
    "\n",
    "### Strategy \n",
    "1. Get the predicted image.\n",
    "2. Extract the white channel\n",
    "3. With threshold of 0.5 classify as either 1 or 0\n",
    "4. Extract white channel from groundtruth\n",
    "5. Crop groundtruth down to the size of predicted image (in-built function)\n",
    "6. Compute accuracy via dice-coefficient\n",
    "7. diff = (predicted!=cropped_ground_truth) i.e white pixels in this image are the one's that were classified wrongly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to save concatenated images (input,groundtruth,predicted(with threshold), diff)\n",
    "\n",
    "def save_img(data_x,white_y,white,diff,name):\n",
    "    fig, ax = plt.subplots(1, 4, sharex=True, sharey=True, figsize=(14,5))\n",
    "    ax[0].imshow(data_x[0,...,-1], aspect=\"auto\", cmap=\"gray\")\n",
    "    ax[1].imshow(white_y[0], aspect=\"auto\",cmap=\"gray\")\n",
    "    ax[2].imshow(white[0], aspect=\"auto\",cmap=\"gray\")\n",
    "    ax[3].imshow(diff[0], aspect=\"auto\",cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].set_title(\"GroundTruth\")\n",
    "    ax[2].set_title(\"Prediction>0.5\")\n",
    "    ax[3].set_title(\"diff\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coeff(pred,y):\n",
    "    pred = pred.reshape(-1,1).astype(np.bool)\n",
    "    y = y.reshape(-1,1).astype(np.bool)\n",
    "    \n",
    "    intersection = np.logical_and(pred,y)\n",
    "    return 2. * intersection.sum() / (pred.sum()+y.sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is where the testing happens\n",
    "\n",
    "def make_predictions(data_provider, outdir,\n",
    "                     save=True, verbose=False,\n",
    "                     printevery=40,\n",
    "                     path = \"./model_val_50_epochs/model.cpkt\"): \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #initialise network. Observe we are using our cutsom myUnet Class\n",
    "    net = myUnet(layers=4, features_root=32, channels=1, n_class=2)\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    \n",
    "    # start tensorflow session\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        # Initialize variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Restore parameters\n",
    "        net.restore(sess, path)\n",
    "        \n",
    "        accuracy= []\n",
    "        \n",
    "        print(\"Printing Mean Accuracy every\", printevery,\"inputs.\")\n",
    "        print(\"Proccessed : Mean Accuracy\")\n",
    "\n",
    "        for i in range(len(data_provider.data_files)):\n",
    "            \n",
    "            # get next x,y pair\n",
    "            x_train,y_train = getnext(data_provider)\n",
    "            \n",
    "            #predict\n",
    "            prediction = net.predict(sess, path, x_train)\n",
    "            \n",
    "            #apply threshold\n",
    "            prediction = ((prediction[...,1]>=0.5).astype('float32')).astype(np.uint8)\n",
    "            \n",
    "            # crop groundtruth\n",
    "            y_cropped = util.crop_to_shape(y_train[...,1], prediction.shape)\n",
    "\n",
    "            #New accuracy metric, % of pixels that match in both\n",
    "            # acc = np.sum(prediction==y_cropped)/(prediction.shape[0]*prediction.shape[1]*prediction.shape[2])\n",
    "            \n",
    "            #dice-coefficient\n",
    "            acc = dice_coeff(prediction[0],y_cropped[0])            \n",
    "\n",
    "            accuracy.append(acc)\n",
    "            \n",
    "            # diff\n",
    "            diff =  ((prediction!=y_cropped).astype('float32')*255).astype(np.uint8)\n",
    "            \n",
    "            # Scale to 255 to display as grayscale\n",
    "            prediction = prediction*255\n",
    "            y_cropped=y_cropped*255\n",
    "            \n",
    "            #filename : files will be sorted according to accuracy\n",
    "            name = outdir+str(acc)[:4]+\"pred\"+str(i)+\".png\"\n",
    "            \n",
    "            if save:\n",
    "                save_img(x_train, y_cropped, prediction,diff,name)\n",
    "            if verbose:\n",
    "                print(\"{0:>4}:\\t{1}\".format(i,acc))\n",
    "            elif (i+1)%printevery == 0 or i==len(data_provider.data_files)-1:\n",
    "                print(\"{0:>4}:\\t{1}\".format(i+1,np.mean(accuracy)))\n",
    "                \n",
    "        \n",
    "        sess.close()\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files used: 599\n",
      "Number of files used: 300\n"
     ]
    }
   ],
   "source": [
    "# generator objects for train and test data\n",
    "\n",
    "train_data_provider = image_util.ImageDataProvider(\"./train/train/*\",data_suffix=\"gray.jpg\",mask_suffix=\"Segmentation.png\")\n",
    "test_data_provider = image_util.ImageDataProvider(\"./test/test/*\",data_suffix=\"gray.jpg\",mask_suffix=\"Segmentation.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for proper testing!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-17 11:58:30,535 Layers 4, features 32, filter size 3x3, pool size: 2x2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_val_50_epochs/model.cpkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-17 11:58:36,676 Restoring parameters from ./model_val_50_epochs/model.cpkt\n",
      "2017-07-17 11:58:36,878 Model restored from file: ./model_val_50_epochs/model.cpkt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Mean Accuracy every 40 inputs.\n",
      "Proccessed : Mean Accuracy\n",
      "  40:\t0.6545202034054005\n",
      "  80:\t0.6402034518354464\n",
      " 120:\t0.6359928063195694\n",
      " 160:\t0.6341383334538531\n",
      " 200:\t0.6233331919255964\n",
      " 240:\t0.6270858860924589\n",
      " 280:\t0.6341078493999311\n",
      " 320:\t0.6455722759868656\n",
      " 360:\t0.646517891658188\n",
      " 400:\t0.6396729157047477\n",
      " 440:\t0.6400368676589882\n",
      " 480:\t0.6414416915816721\n",
      " 520:\t0.6450608615408485\n",
      " 560:\t0.6418905066460421\n",
      " 599:\t0.6463811461730673\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = make_predictions(train_data_provider,\"./pred/train_pred_dice/\",save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUuMMeklq3P8BwkgaXVf2s0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7919250cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accuracy = np.asarray(train_accuracy)\n",
    "a,b,hist = plt.hist(train_accuracy,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to view actual counts in histogram\n",
    "def histdata(accuracy):\n",
    "    numvalues, x = np.histogram(accuracy, bins=np.arange(0,1.1,0.1))\n",
    "    print(\"Accuracy  :   No. of predictions\\n\")\n",
    "    for i in range(len(numvalues)):\n",
    "        print(\"{:03.1f} - {:03.1f} : {:>4}\".format(x[i],x[i+1],numvalues[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  :   No. of predictions\n",
      "\n",
      "0.0 - 0.1 :   17\n",
      "0.1 - 0.2 :   50\n",
      "0.2 - 0.3 :   37\n",
      "0.3 - 0.4 :   34\n",
      "0.4 - 0.5 :   42\n",
      "0.5 - 0.6 :   45\n",
      "0.6 - 0.7 :   44\n",
      "0.7 - 0.8 :   88\n",
      "0.8 - 0.9 :  101\n",
      "0.9 - 1.0 :  141\n"
     ]
    }
   ],
   "source": [
    "histdata(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.646381146173\n",
      "Accuracy > 0.8 : 40.4006677796 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy:\",np.mean(train_accuracy))\n",
    "print(\"Accuracy > 0.8 :\", np.mean(train_accuracy>=0.8)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model predicts 64.63% of the pixels correctly per image. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualise a bit more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-17 14:39:17,858 Layers 4, features 32, filter size 3x3, pool size: 2x2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_val_50_epochs/model.cpkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-17 14:39:24,134 Restoring parameters from ./model_val_50_epochs/model.cpkt\n",
      "2017-07-17 14:39:24,826 Model restored from file: ./model_val_50_epochs/model.cpkt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Mean Accuracy every 40 inputs.\n",
      "Proccessed : Mean Accuracy\n",
      "  40:\t0.6126622799152859\n",
      "  80:\t0.6029529290574199\n",
      " 120:\t0.6141805412575029\n",
      " 160:\t0.6078714718116836\n",
      " 200:\t0.6052577679677759\n",
      " 240:\t0.6145155095454353\n",
      " 280:\t0.6224489263093678\n",
      " 300:\t0.6241777392384148\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = make_predictions(test_data_provider,\"./pred/test_pred_dice/\",save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f791afcc668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_accuracy = np.asarray(test_accuracy)\n",
    "a,b,hist = plt.hist(test_accuracy,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  :   No. of predictions\n",
      "\n",
      "0.0 - 0.1 :    4\n",
      "0.1 - 0.2 :   15\n",
      "0.2 - 0.3 :   19\n",
      "0.3 - 0.4 :   26\n",
      "0.4 - 0.5 :   33\n",
      "0.5 - 0.6 :   32\n",
      "0.6 - 0.7 :   38\n",
      "0.7 - 0.8 :   35\n",
      "0.8 - 0.9 :   55\n",
      "0.9 - 1.0 :   43\n"
     ]
    }
   ],
   "source": [
    "histdata(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.624177739238\n",
      "Accuracy > 0.8 : 32.6666666667 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy:\",np.mean(test_accuracy))\n",
    "print(\"Accuracy > 0.8 :\", np.mean(test_accuracy>=0.8)*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
